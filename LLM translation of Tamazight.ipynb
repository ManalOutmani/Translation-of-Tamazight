{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b64d16b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3347c242f4d94de4825ac98dd8c0defb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login()  # Will prompt for your token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ef70ef86",
   "metadata": {},
   "outputs": [],
   "source": [
    "!del tamazight_model.pt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b76a776c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model not found. Training new model...\n",
      "\n",
      "============================================================\n",
      "IMPORTANT: Beni-Mellal dataset requires authentication!\n",
      "============================================================\n",
      "\n",
      "If you see authentication errors, run this command in terminal:\n",
      "  huggingface-cli login\n",
      "\n",
      "Or authenticate in Python:\n",
      "  from huggingface_hub import login\n",
      "  login()\n",
      "\n",
      "Alternatively, use 'weblate' dataset which doesn't require auth:\n",
      "  load_and_preprocess_dataset(dataset_choice='weblate')\n",
      "============================================================\n",
      "\n",
      "Loading Weblate-Translations dataset...\n",
      "✓ Loaded 1138 examples from Weblate-Translations\n",
      "\n",
      "Loading Beni-Mellal-Tamazight dataset...\n",
      "Note: This dataset may require authentication.\n",
      "If you haven't logged in, run: huggingface-cli login\n",
      "✓ Dataset loaded successfully!\n",
      "  Available splits: ['train']\n",
      "\n",
      "Processing train split...\n",
      "  Columns: ['English', 'Tamazight']\n",
      "  Size: 2322 examples\n",
      "  First example keys: ['English', 'Tamazight']\n",
      "  Added 2321 examples from train\n",
      "\n",
      "✓ Total added from Beni-Mellal-Tamazight: 2321 examples\n",
      "✓ Total examples: 3459\n",
      "\n",
      "============================================================\n",
      "Final dataset size: 3459 translation pairs\n",
      "============================================================\n",
      "\n",
      "Sample translations:\n",
      "\n",
      "  [1] EN: Search\n",
      "      TZM (Latin): rzu\n",
      "      TZM (Tifinagh): ⵔⵣⵓ\n",
      "\n",
      "  [2] EN: edit\n",
      "      TZM (Latin): sggd\n",
      "      TZM (Tifinagh): ⵙⴳⴳⴷ\n",
      "\n",
      "  [3] EN: Google Play</a>\n",
      "      TZM (Latin): gugl ⵒlay</a>\n",
      "      TZM (Tifinagh): ⴳⵓⴳⵍ ⵒⵍⴰⵢ</ⴰ>\n",
      "\n",
      "  [4] EN: Send file(s)\n",
      "      TZM (Latin): azn afailu(ifaylutn)\n",
      "      TZM (Tifinagh): ⴰⵣⵏ ⴰⴼⴰⵉⵍⵓ(ⵉⴼⴰⵢⵍⵓⵜⵏ)\n",
      "\n",
      "  [5] EN: Send SMS\n",
      "      TZM (Latin): azn SMS\n",
      "      TZM (Tifinagh): ⴰⵣⵏ ⵙⵎⵙ\n",
      "Using device: cpu\n",
      "Building vocabularies...\n",
      "Source vocab size: 106\n",
      "Target vocab size: 148\n",
      "\n",
      "Starting training...\n",
      "Epoch [1/10], Step [50/109], Loss: 2.9518\n",
      "Epoch [1/10], Step [100/109], Loss: 2.6867\n",
      "Epoch [1/10] completed. Average Loss: 3.0716\n",
      "Epoch [2/10], Step [50/109], Loss: 2.6428\n",
      "Epoch [2/10], Step [100/109], Loss: 2.4207\n",
      "Epoch [2/10] completed. Average Loss: 2.5257\n",
      "Epoch [3/10], Step [50/109], Loss: 2.2304\n",
      "Epoch [3/10], Step [100/109], Loss: 2.1543\n",
      "Epoch [3/10] completed. Average Loss: 2.2527\n",
      "Epoch [4/10], Step [50/109], Loss: 1.9253\n",
      "Epoch [4/10], Step [100/109], Loss: 1.9367\n",
      "Epoch [4/10] completed. Average Loss: 2.0343\n",
      "Epoch [5/10], Step [50/109], Loss: 1.6662\n",
      "Epoch [5/10], Step [100/109], Loss: 1.8993\n",
      "Epoch [5/10] completed. Average Loss: 1.8409\n",
      "Epoch [6/10], Step [50/109], Loss: 1.6345\n",
      "Epoch [6/10], Step [100/109], Loss: 1.5523\n",
      "Epoch [6/10] completed. Average Loss: 1.6588\n",
      "Epoch [7/10], Step [50/109], Loss: 1.3532\n",
      "Epoch [7/10], Step [100/109], Loss: 1.4843\n",
      "Epoch [7/10] completed. Average Loss: 1.4877\n",
      "Epoch [8/10], Step [50/109], Loss: 1.3295\n",
      "Epoch [8/10], Step [100/109], Loss: 1.5631\n",
      "Epoch [8/10] completed. Average Loss: 1.3293\n",
      "Epoch [9/10], Step [50/109], Loss: 1.2225\n",
      "Epoch [9/10], Step [100/109], Loss: 1.1908\n",
      "Epoch [9/10] completed. Average Loss: 1.1880\n",
      "Epoch [10/10], Step [50/109], Loss: 1.1564\n",
      "Epoch [10/10], Step [100/109], Loss: 1.0728\n",
      "Epoch [10/10] completed. Average Loss: 1.0406\n",
      "\n",
      "Training complete!\n",
      "Model saved to ./tamazight_model.pt\n",
      "\n",
      "=== Testing Translation ===\n",
      "English: Hello\n",
      "Tamazight (Latin): afran.\n",
      "Tamazight (Tifinagh): ⴰⴼⵔⴰⵏ.\n",
      "\n",
      "English: How are you?\n",
      "Tamazight (Latin): mayd-d-idda?\n",
      "Tamazight (Tifinagh): ⵎⴰⵢⴷ-ⴷ-ⵉⴷⴷⴰ?\n",
      "\n",
      "English: Thank you\n",
      "Tamazight (Latin): aynag ttinin meddn.\n",
      "Tamazight (Tifinagh): ⴰⵢⵏⴰⴳ ⵜⵜⵉⵏⵉⵏ ⵎⴻⴷⴷⵏ.\n",
      "\n",
      "English: Good morning\n",
      "Tamazight (Latin): ddu ghr ssuq\n",
      "Tamazight (Tifinagh): ⴷⴷⵓ ⴳⵀⵔ ⵙⵙⵓⵇ\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "# Tifinagh transliteration mapping\n",
    "LATIN_TO_TIFINAGH = {\n",
    "    'a': 'ⴰ', 'b': 'ⴱ', 'c': 'ⵛ', 'č': 'ⵞ', 'd': 'ⴷ', 'ḍ': 'ⴹ',\n",
    "    'e': 'ⴻ', 'ɛ': 'ⵄ', 'f': 'ⴼ', 'g': 'ⴳ', 'ǧ': 'ⴵ', 'ɣ': 'ⵖ',\n",
    "    'h': 'ⵀ', 'ḥ': 'ⵃ', 'i': 'ⵉ', 'j': 'ⵊ', 'k': 'ⴽ', 'l': 'ⵍ',\n",
    "    'm': 'ⵎ', 'n': 'ⵏ', 'q': 'ⵇ', 'r': 'ⵔ', 'ṛ': 'ⵕ', 's': 'ⵙ',\n",
    "    'ṣ': 'ⵚ', 't': 'ⵜ', 'ṭ': 'ⵟ', 'u': 'ⵓ', 'w': 'ⵡ', 'x': 'ⵅ',\n",
    "    'y': 'ⵢ', 'z': 'ⵣ', 'ẓ': 'ⵥ',\n",
    "    'A': 'ⴰ', 'B': 'ⴱ', 'C': 'ⵛ', 'Č': 'ⵞ', 'D': 'ⴷ', 'Ḍ': 'ⴹ',\n",
    "    'E': 'ⴻ', 'Ɛ': 'ⵄ', 'F': 'ⴼ', 'G': 'ⴳ', 'Ǧ': 'ⴵ', 'Ɣ': 'ⵖ',\n",
    "    'H': 'ⵀ', 'Ḥ': 'ⵃ', 'I': 'ⵉ', 'J': 'ⵊ', 'K': 'ⴽ', 'L': 'ⵍ',\n",
    "    'M': 'ⵎ', 'N': 'ⵏ', 'Q': 'ⵇ', 'R': 'ⵔ', 'Ṛ': 'ⵕ', 'S': 'ⵙ',\n",
    "    'Ṣ': 'ⵚ', 'T': 'ⵜ', 'Ṭ': 'ⵟ', 'U': 'ⵓ', 'W': 'ⵡ', 'X': 'ⵅ',\n",
    "    'Y': 'ⵢ', 'Z': 'ⵣ', 'Ẓ': 'ⵥ'\n",
    "}\n",
    "\n",
    "TIFINAGH_TO_LATIN = {v: k.lower() for k, v in LATIN_TO_TIFINAGH.items()}\n",
    "\n",
    "def tifinagh_to_latin(text):\n",
    "    \"\"\"Convert Tifinagh script to Latin script\"\"\"\n",
    "    result = []\n",
    "    for char in text:\n",
    "        result.append(TIFINAGH_TO_LATIN.get(char, char))\n",
    "    return ''.join(result)\n",
    "\n",
    "def latin_to_tifinagh(text):\n",
    "    \"\"\"Convert Latin script to Tifinagh script\"\"\"\n",
    "    sorted_chars = sorted(LATIN_TO_TIFINAGH.keys(), key=len, reverse=True)\n",
    "    result = text\n",
    "    for latin_char in sorted_chars:\n",
    "        result = result.replace(latin_char, LATIN_TO_TIFINAGH[latin_char])\n",
    "    return result\n",
    "\n",
    "class Vocabulary:\n",
    "    def __init__(self):\n",
    "        self.char2idx = {'<PAD>': 0, '<SOS>': 1, '<EOS>': 2, '<UNK>': 3}\n",
    "        self.idx2char = {0: '<PAD>', 1: '<SOS>', 2: '<EOS>', 3: '<UNK>'}\n",
    "        self.n_chars = 4\n",
    "    \n",
    "    def add_char(self, char):\n",
    "        if char not in self.char2idx:\n",
    "            self.char2idx[char] = self.n_chars\n",
    "            self.idx2char[self.n_chars] = char\n",
    "            self.n_chars += 1\n",
    "    \n",
    "    def build_vocab(self, texts):\n",
    "        for text in texts:\n",
    "            for char in text:\n",
    "                self.add_char(char)\n",
    "\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, source_texts, target_texts, src_vocab, tgt_vocab, max_len=100):\n",
    "        self.source_texts = source_texts\n",
    "        self.target_texts = target_texts\n",
    "        self.src_vocab = src_vocab\n",
    "        self.tgt_vocab = tgt_vocab\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.source_texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        src = self.source_texts[idx].lower()\n",
    "        tgt = self.target_texts[idx]\n",
    "        \n",
    "        # Encode source\n",
    "        src_indices = [self.src_vocab.char2idx.get(c, 3) for c in src]\n",
    "        src_indices = src_indices[:self.max_len]\n",
    "        \n",
    "        # Encode target\n",
    "        tgt_indices = [self.tgt_vocab.char2idx['<SOS>']]\n",
    "        tgt_indices += [self.tgt_vocab.char2idx.get(c, 3) for c in tgt]\n",
    "        tgt_indices += [self.tgt_vocab.char2idx['<EOS>']]\n",
    "        tgt_indices = tgt_indices[:self.max_len]\n",
    "        \n",
    "        return (torch.tensor(src_indices), torch.tensor(tgt_indices))\n",
    "\n",
    "def collate_fn(batch):\n",
    "    src_batch, tgt_batch = [], []\n",
    "    for src, tgt in batch:\n",
    "        src_batch.append(src)\n",
    "        tgt_batch.append(tgt)\n",
    "    \n",
    "    src_batch = nn.utils.rnn.pad_sequence(src_batch, padding_value=0, batch_first=True)\n",
    "    tgt_batch = nn.utils.rnn.pad_sequence(tgt_batch, padding_value=0, batch_first=True)\n",
    "    \n",
    "    return src_batch, tgt_batch\n",
    "\n",
    "class Seq2SeqModel(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, embed_size=256, hidden_size=512, num_layers=2, dropout=0.3):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder_embed = nn.Embedding(src_vocab_size, embed_size, padding_idx=0)\n",
    "        self.decoder_embed = nn.Embedding(tgt_vocab_size, embed_size, padding_idx=0)\n",
    "        \n",
    "        self.encoder = nn.LSTM(embed_size, hidden_size, num_layers, \n",
    "                              batch_first=True, dropout=dropout if num_layers > 1 else 0, bidirectional=True)\n",
    "        \n",
    "        self.decoder = nn.LSTM(embed_size, hidden_size * 2, num_layers, \n",
    "                              batch_first=True, dropout=dropout if num_layers > 1 else 0)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_size * 2, tgt_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, src, tgt):\n",
    "        # Encode\n",
    "        src_embed = self.dropout(self.encoder_embed(src))\n",
    "        encoder_out, (hidden, cell) = self.encoder(src_embed)\n",
    "        \n",
    "        # Prepare decoder hidden state\n",
    "        hidden = hidden.view(self.encoder.num_layers, 2, src.size(0), -1)\n",
    "        hidden = torch.cat([hidden[:, 0], hidden[:, 1]], dim=2)\n",
    "        \n",
    "        cell = cell.view(self.encoder.num_layers, 2, src.size(0), -1)\n",
    "        cell = torch.cat([cell[:, 0], cell[:, 1]], dim=2)\n",
    "        \n",
    "        # Decode\n",
    "        tgt_embed = self.dropout(self.decoder_embed(tgt))\n",
    "        decoder_out, _ = self.decoder(tgt_embed, (hidden, cell))\n",
    "        \n",
    "        output = self.fc(decoder_out)\n",
    "        return output\n",
    "\n",
    "def load_and_preprocess_dataset(dataset_choice='both'):\n",
    "    \"\"\"Load datasets from Hugging Face\n",
    "    \n",
    "    Args:\n",
    "        dataset_choice: 'both', 'weblate', or 'beni-mellal'\n",
    "    \n",
    "    Returns:\n",
    "        source_texts, target_texts: Lists of English and Tamazight (Latin) texts\n",
    "    \"\"\"\n",
    "    from datasets import load_dataset\n",
    "    \n",
    "    source_texts = []\n",
    "    target_texts = []\n",
    "    \n",
    "    # Dataset 1: Weblate-Translations\n",
    "    if dataset_choice in ['both', 'weblate']:\n",
    "        print(\"Loading Weblate-Translations dataset...\")\n",
    "        try:\n",
    "            ds1 = load_dataset(\"Tamazight-NLP/Weblate-Translations\", \"en-ber\")\n",
    "            \n",
    "            for example in ds1['train']:\n",
    "                source_texts.append(example['source_string'])\n",
    "                # Convert Tifinagh to Latin\n",
    "                target_texts.append(tifinagh_to_latin(example['target_string']))\n",
    "            \n",
    "            print(f\"✓ Loaded {len(source_texts)} examples from Weblate-Translations\")\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error loading Weblate-Translations: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "    \n",
    "    # Dataset 2: Beni-Mellal-Tamazight\n",
    "    if dataset_choice in ['both', 'beni-mellal']:\n",
    "        print(\"\\nLoading Beni-Mellal-Tamazight dataset...\")\n",
    "        print(\"Note: This dataset may require authentication.\")\n",
    "        print(\"If you haven't logged in, run: huggingface-cli login\")\n",
    "        \n",
    "        try:\n",
    "            # Try loading with token if available\n",
    "            \n",
    "\n",
    "            ds2 = load_dataset(\n",
    "                \"parquet\",\n",
    "                data_files={\n",
    "                    \"train\": \"hf://datasets/Tamazight-NLP/Beni-Mellal-Tamazight/data/train-00000-of-00001.parquet\"\n",
    "                }\n",
    "            )\n",
    "            ds2=ds2.remove_columns([\"Audio\"])\n",
    "\n",
    "            \n",
    "            initial_count = len(source_texts)\n",
    "            \n",
    "            # Check what splits are available\n",
    "            print(f\"✓ Dataset loaded successfully!\")\n",
    "            print(f\"  Available splits: {list(ds2.keys())}\")\n",
    "            \n",
    "            # Iterate through all splits (train, test, validation, etc.)\n",
    "            for split_name in ds2.keys():\n",
    "                print(f\"\\nProcessing {split_name} split...\")\n",
    "                split_data = ds2[split_name]\n",
    "                \n",
    "                # Check column names\n",
    "                if len(split_data) > 0:\n",
    "                    print(f\"  Columns: {split_data.column_names}\")\n",
    "                    print(f\"  Size: {len(split_data)} examples\")\n",
    "                    \n",
    "                    # Show first example to debug\n",
    "                    print(f\"  First example keys: {list(split_data[0].keys())}\")\n",
    "                \n",
    "                added_in_split = 0\n",
    "                for example in split_data:\n",
    "                    # Handle different possible column names\n",
    "                    english_text = None\n",
    "                    tamazight_text = None\n",
    "                    \n",
    "                    # Try to get English text\n",
    "                    if 'English' in example:\n",
    "                        english_text = example['English']\n",
    "                    elif 'english' in example:\n",
    "                        english_text = example['english']\n",
    "                    elif 'source' in example:\n",
    "                        english_text = example['source']\n",
    "                    elif 'en' in example:\n",
    "                        english_text = example['en']\n",
    "                    \n",
    "                    # Try to get Tamazight text\n",
    "                    if 'tamazight' in example:\n",
    "                        tamazight_text = example['tamazight']\n",
    "                    elif 'Tamazight' in example:\n",
    "                        tamazight_text = example['Tamazight']\n",
    "                    elif 'target' in example:\n",
    "                        tamazight_text = example['target']\n",
    "                    elif 'tzm' in example:\n",
    "                        tamazight_text = example['tzm']\n",
    "                    elif 'ber' in example:\n",
    "                        tamazight_text = example['ber']\n",
    "                    \n",
    "                    # Add to dataset if both are valid\n",
    "                    if english_text and tamazight_text:\n",
    "                        # Clean whitespace\n",
    "                        english_text = str(english_text).strip()\n",
    "                        tamazight_text = str(tamazight_text).strip()\n",
    "                        \n",
    "                        if english_text and tamazight_text:\n",
    "                            source_texts.append(english_text)\n",
    "                            # Convert Tifinagh to Latin\n",
    "                            target_texts.append(tifinagh_to_latin(tamazight_text))\n",
    "                            added_in_split += 1\n",
    "                \n",
    "                print(f\"  Added {added_in_split} examples from {split_name}\")\n",
    "            \n",
    "            added_count = len(source_texts) - initial_count\n",
    "            print(f\"\\n✓ Total added from Beni-Mellal-Tamazight: {added_count} examples\")\n",
    "            print(f\"✓ Total examples: {len(source_texts)}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error loading Beni-Mellal-Tamazight: {e}\")\n",
    "            print(f\"  Error type: {type(e).__name__}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            \n",
    "            if dataset_choice == 'beni-mellal':\n",
    "                print(\"\\n  FALLBACK: Beni-Mellal dataset failed. Using Weblate instead...\")\n",
    "                if len(source_texts) == 0:\n",
    "                    print(\"  Loading Weblate-Translations as fallback...\")\n",
    "                    try:\n",
    "                        ds1 = load_dataset(\"Tamazight-NLP/Weblate-Translations\", \"en-ber\")\n",
    "                        for example in ds1['train']:\n",
    "                            source_texts.append(example['source_string'])\n",
    "                            target_texts.append(tifinagh_to_latin(example['target_string']))\n",
    "                        print(f\"✓ Loaded {len(source_texts)} examples from Weblate-Translations (fallback)\")\n",
    "                    except Exception as fallback_error:\n",
    "                        print(f\"✗ Fallback also failed: {fallback_error}\")\n",
    "            else:\n",
    "                print(\"  Continuing with Weblate-Translations only...\")\n",
    "    \n",
    "    if len(source_texts) == 0:\n",
    "        raise ValueError(\n",
    "            \"No data loaded! Please check:\\n\"\n",
    "            \"1. Internet connection\\n\"\n",
    "            \"2. Hugging Face authentication (run: huggingface-cli login)\\n\"\n",
    "            \"3. Dataset availability\\n\"\n",
    "            \"Try using dataset_choice='weblate' which doesn't require authentication.\"\n",
    "        )\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Final dataset size: {len(source_texts)} translation pairs\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Show some examples from each dataset\n",
    "    print(\"\\nSample translations:\")\n",
    "    for i in range(min(5, len(source_texts))):\n",
    "        en_preview = source_texts[i][:60] if len(source_texts[i]) > 60 else source_texts[i]\n",
    "        tz_preview = target_texts[i][:60] if len(target_texts[i]) > 60 else target_texts[i]\n",
    "        print(f\"\\n  [{i+1}] EN: {en_preview}\")\n",
    "        print(f\"      TZM (Latin): {tz_preview}\")\n",
    "        print(f\"      TZM (Tifinagh): {latin_to_tifinagh(tz_preview)}\")\n",
    "    \n",
    "    return source_texts, target_texts\n",
    "\n",
    "def train_model(source_texts, target_texts, epochs=10, batch_size=32, max_samples=None):\n",
    "    \"\"\"Train the Seq2Seq model from scratch\"\"\"\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Limit samples for quick testing\n",
    "    if max_samples:\n",
    "        source_texts = source_texts[:max_samples]\n",
    "        target_texts = target_texts[:max_samples]\n",
    "        print(f\"Training on {len(source_texts)} samples\")\n",
    "    \n",
    "    # Build vocabularies\n",
    "    print(\"Building vocabularies...\")\n",
    "    src_vocab = Vocabulary()\n",
    "    tgt_vocab = Vocabulary()\n",
    "    \n",
    "    src_vocab.build_vocab(source_texts)\n",
    "    tgt_vocab.build_vocab(target_texts)\n",
    "    \n",
    "    print(f\"Source vocab size: {src_vocab.n_chars}\")\n",
    "    print(f\"Target vocab size: {tgt_vocab.n_chars}\")\n",
    "    \n",
    "    # Create dataset\n",
    "    dataset = TranslationDataset(source_texts, target_texts, src_vocab, tgt_vocab)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    \n",
    "    # Initialize model\n",
    "    model = Seq2SeqModel(src_vocab.n_chars, tgt_vocab.n_chars).to(device)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    # Training loop\n",
    "    print(\"\\nStarting training...\")\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for i, (src, tgt) in enumerate(dataloader):\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "            \n",
    "            # Target input (all but last token)\n",
    "            tgt_input = tgt[:, :-1]\n",
    "            # Target output (all but first token)\n",
    "            tgt_output = tgt[:, 1:]\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            output = model(src, tgt_input)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = criterion(output.reshape(-1, tgt_vocab.n_chars), tgt_output.reshape(-1))\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            if (i + 1) % 50 == 0:\n",
    "                print(f\"Epoch [{epoch+1}/{epochs}], Step [{i+1}/{len(dataloader)}], Loss: {loss.item():.4f}\")\n",
    "        \n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}] completed. Average Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    print(\"\\nTraining complete!\")\n",
    "    \n",
    "    # Save model and vocabularies\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'src_vocab': src_vocab,\n",
    "        'tgt_vocab': tgt_vocab,\n",
    "        'model_config': {\n",
    "            'src_vocab_size': src_vocab.n_chars,\n",
    "            'tgt_vocab_size': tgt_vocab.n_chars\n",
    "        }\n",
    "    }, './tamazight_model.pt')\n",
    "    \n",
    "    print(\"Model saved to ./tamazight_model.pt\")\n",
    "    return model, src_vocab, tgt_vocab\n",
    "\n",
    "def translate_to_latin(text, model_path='./tamazight_model.pt', max_len=100):\n",
    "    \"\"\"Translate English to Tamazight (Latin script)\"\"\"\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Load model and vocabularies\n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    src_vocab = checkpoint['src_vocab']\n",
    "    tgt_vocab = checkpoint['tgt_vocab']\n",
    "    \n",
    "    model = Seq2SeqModel(src_vocab.n_chars, tgt_vocab.n_chars).to(device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval()\n",
    "    \n",
    "    # Encode input\n",
    "    text = text.lower()\n",
    "    src_indices = [src_vocab.char2idx.get(c, 3) for c in text]\n",
    "    src_tensor = torch.tensor([src_indices]).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Encode\n",
    "        src_embed = model.encoder_embed(src_tensor)\n",
    "        encoder_out, (hidden, cell) = model.encoder(src_embed)\n",
    "        \n",
    "        # Prepare hidden states\n",
    "        hidden = hidden.view(model.encoder.num_layers, 2, 1, -1)\n",
    "        hidden = torch.cat([hidden[:, 0], hidden[:, 1]], dim=2)\n",
    "        cell = cell.view(model.encoder.num_layers, 2, 1, -1)\n",
    "        cell = torch.cat([cell[:, 0], cell[:, 1]], dim=2)\n",
    "        \n",
    "        # Decode\n",
    "        decoder_input = torch.tensor([[tgt_vocab.char2idx['<SOS>']]]).to(device)\n",
    "        decoded_chars = []\n",
    "        \n",
    "        for _ in range(max_len):\n",
    "            tgt_embed = model.decoder_embed(decoder_input)\n",
    "            decoder_out, (hidden, cell) = model.decoder(tgt_embed, (hidden, cell))\n",
    "            output = model.fc(decoder_out)\n",
    "            \n",
    "            # Get predicted character\n",
    "            pred_idx = output.argmax(2).item()\n",
    "            \n",
    "            # Stop if EOS\n",
    "            if pred_idx == tgt_vocab.char2idx['<EOS>']:\n",
    "                break\n",
    "            \n",
    "            # Add to result\n",
    "            if pred_idx not in [0, 1, 2]:  # Skip PAD, SOS, EOS\n",
    "                decoded_chars.append(tgt_vocab.idx2char[pred_idx])\n",
    "            \n",
    "            # Next input\n",
    "            decoder_input = torch.tensor([[pred_idx]]).to(device)\n",
    "    \n",
    "    return ''.join(decoded_chars)\n",
    "\n",
    "def translate_and_convert(text, model_path='./tamazight_model.pt'):\n",
    "    \"\"\"Translate English to Tamazight and convert to Tifinagh\"\"\"\n",
    "    latin_result = translate_to_latin(text, model_path)\n",
    "    return latin_to_tifinagh(latin_result)\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    import os\n",
    "    import shutil\n",
    "    \n",
    "    # Force retrain option\n",
    "    FORCE_RETRAIN = False  \n",
    "    \n",
    "    if FORCE_RETRAIN and os.path.exists('./tamazight_model.pt'):\n",
    "        print(\"Deleting old model to retrain...\")\n",
    "        os.remove('./tamazight_model.pt')\n",
    "    \n",
    "    if not os.path.exists('./tamazight_model.pt'):\n",
    "        print(\"Model not found. Training new model...\")\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"IMPORTANT: Beni-Mellal dataset requires authentication!\")\n",
    "        print(\"=\"*60)\n",
    "        print(\"\\nIf you see authentication errors, run this command in terminal:\")\n",
    "        print(\"  huggingface-cli login\")\n",
    "        print(\"\\nOr authenticate in Python:\")\n",
    "        print(\"  from huggingface_hub import login\")\n",
    "        print(\"  login()\")\n",
    "        print(\"\\nAlternatively, use 'weblate' dataset which doesn't require auth:\")\n",
    "        print(\"  load_and_preprocess_dataset(dataset_choice='weblate')\")\n",
    "        print(\"=\"*60 + \"\\n\")\n",
    "        \n",
    "        # Load ONLY Beni-Mellal dataset\n",
    "        #source_texts, target_texts = load_and_preprocess_dataset(dataset_choice='beni-mellal')\n",
    "        source_texts, target_texts = load_and_preprocess_dataset(dataset_choice='both')\n",
    "        # Train model\n",
    "        # For quick testing: max_samples=1000, epochs=5\n",
    "        # For better quality: max_samples=None, epochs=10-15\n",
    "        model, src_vocab, tgt_vocab = train_model(\n",
    "            source_texts, target_texts, \n",
    "            epochs=10, \n",
    "            batch_size=32,\n",
    "            max_samples=None  # Set to 1000 for quick testing\n",
    "        )\n",
    "    else:\n",
    "        print(\"Model found. Skipping training.\")\n",
    "      \n",
    "    \n",
    "    # Test translation\n",
    "    print(\"\\n=== Testing Translation ===\")\n",
    "    test_texts = [\"Hello\", \"How are you?\", \"Thank you\", \"Good morning\"]\n",
    "    \n",
    "    for test_text in test_texts:\n",
    "        latin = translate_to_latin(test_text)\n",
    "        tifinagh = translate_and_convert(test_text)\n",
    "        \n",
    "        print(f\"English: {test_text}\")\n",
    "        print(f\"Tamazight (Latin): {latin}\")\n",
    "        print(f\"Tamazight (Tifinagh): {tifinagh}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "97ab66ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'imshinnax a-š-i3jbn'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate_to_latin(\"Hello how are you\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a815072b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
